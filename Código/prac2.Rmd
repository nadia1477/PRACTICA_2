---
title: "Práctica 2. Limpieza y resolución de los datos"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(tidyverse)
```

https://www.kaggle.com/hiteshp/head-start-for-data-scientist

## Índice

## 1. DETALLES DE LA ACTIVIDAD

### 1.1. Presentación

En esta práctica se elabora un caso práctico orientado a aprender a identificar los datos relevantes para un proyecto analítico y usar las herramientas de integración, limpieza, validación y análisis de las mismas. 

### 1.2. Competencias 

En esta práctica se desarrollan las siguientes competencias del Máster de Data Science: 

- Capacidad de analizar un problema en el nivel de abstracción adecuado a cada situación y aplicar las habilidades y conocimientos adquiridos para abordarlo y resolverlo. 
- Capacidad para aplicar las técnicas específicas de tratamiento de datos (integración, transformación, limpieza y validación) para su posterior análisis. 

### 1.3 Objetivos

Los objetivos concretos de esta práctica son: 

- Aprender a aplicar los conocimientos adquiridos y su capacidad de resolución de problemas en entornos nuevos o poco conocidos dentro de contextos más amplios o multidisciplinares. 
- Saber identificar los datos relevantes y los tratamientos necesarios (integración, limpieza y validación) para llevar a cabo un proyecto analítico. 
- Aprender a analizar los datos adecuadamente para abordar la información contenida en los datos. 
- Identificar la mejor representación de los resultados para aportar conclusiones sobre el problema planteado en el proceso analítico. 
- Actuar con los principios éticos y legales relacionados con la manipulación de datos en función del ámbito de aplicación. 
- Desarrollar las habilidades de aprendizaje que les permitan continuar estudiando de un modo que tendrá que ser en gran medida autodirigido o autónomo. 
- Desarrollar la capacidad de búsqueda, gestión y uso de información y recursos en el ámbito de la ciencia de datos. 

## 2. RESOLUCIÓN

### 2.1. Descripción de la base de datos.  ¿Por qué es importante y qué pregunta/problema pretende responder?

La base de datos que se ha analizado en esta práctica se títula **Titanic: Machine Learning from Disaster** (https://www.kaggle.com/c/titanic) [!https://www.kaggle.com/c/titanic].

Los datos de este archivo se encuentran divididos en dos bases de datos, uno de ellos es la base de datos de entrenamiento (*train.csv*) y la otra base de datos es la de prueba (*test.csv*).

El archivo de entrenamiento es el que usaremos para crear el modelo de machine learning. En esta base de datos tenemos la variable  que aporta la información sobre si el pasajero sobrevivió o no. El modelo que crearemos se va a basar en las demás variables que tengamos, y el objetivo es que el modelo sea capaz de predecir si un pasajero sobrevivió o no. Para comprobar la eficacia de nuestro modelo usaremos la otra base de datos, la de prueba. En esta base de datos no tenemos la información de si el pasajero sobrevivió o no, sino que lo debemos predecir con nuestro modelo.

Resumiendo, el 14 de Abril de 1992 el Titanic chocó con un iceberg y se llevo aproximadamente a 1500 de sus pasajeros y tripulación a las profundidades del oceano. Este incidente ha sido considerado uno de los desastres marinos más importantes en tiempos de paz, y a causa de dicho indicente se actualizaron o renovaron numerosas políticas de seguridad. Sin embargo, existen numerosas voces que dicen que hubo circunstancias que hicieron que hubiera un desproporcionada cantidad de muertos. El objetivo de analizar esta base de datos es explorar los factores que tuvieron relación con el hecho de que una persona sobreviviera o no a la catastrofe del Titanic.


**Descríbamos la base de datos.**

Esta base de datos tiene 891 observaciones y 12 variables.

- **PassengerId**:Variable que aporta el código de identificación de los pasajeros.
- **Survived**: Variable dicotómica que indica si el pasajero sobrevivión (1) o no sobrevivió (0).
- **Pclass**: Variable categórica que indica si los pasajeros tenían tickets de primera clase (1), segunda clase (2) o tercera clase (3). Obviamente los tickets más caros eran los de primera clase, seguidos de los de segunda clase y finalmente los de tercera clase.
- **Name**: Variable de tipo cadena con el nombre y apellidos de los pasajeros.
- **Sex**: Variable dicotómica que indica si el pasajero era un hombre (1) o una mujer (2).
- **Age**: Variable numérica que indica la edad de los pasajeros en años. En el caso de ser personas con menos de un años se indica la fracción (con un décimal), en caso de tener más de un año se utilizan números enteros.
- **SibSp**: Variable numérica (números enteros) que indicaba el número de familiares/cónyuges que tenían los pasajeros a bordo del Titanic.
- **Parch**: Variable numérica (números enteros) que indicaba el número de hijos/padres que tenían los pasajeros a bordo del Titanic.
- **Ticket**: Código/Número del tiket (podía haber más de un pasajero con el mismo número).
- **Fare**: Variable numérica que indica el precio del pase del pasajero.
- **Cabin**: Código que identifica la cabina del pasajero.
- **Embarked**: Variable categórica con tres niveles que indica la puerta por la cual embarcaron (puerta "C", "Q", o "S"). 




### 2.2. Integración y selección de los datos de interés a analizar. 

En primer lugar integraremos las dos bases de datos que tenemos (*train* y *test*). Antes de hacer esto, añadiremos una variable en cada base de datos para que identifique si la observación pertenece a la base *train* o a la base *test*. esta nueva variable la denominaremos *source*.

```{r}
# cargar bases de datos
train <- read.csv("train.csv")
test <- read.csv("test.csv")
```

```{r}
# Añadimos la variable source
train$source <- "train"
test$source <- "test"
```

Antes de "juntar" las dos bases de datos, necesitamos que ambas bases de datos tengan exactamente las mismas variables por lo que tenemos que añadir la variable *Survived* a la base de datos *test*. Como no sabemos el valor podemos poner "NA".

```{r}
test$Survived <- NA
```

Ahora sí, las dos bases de datos tienen las mismas variables así que las podemos juntar por ejemplo con la función **rbind()**.

```{r}
complete_df <- rbind(train, test)
complete_df <- as_tibble(complete_df)
```

Veamos que es lo que tenemos en la base de datos completa. Para tener una idea general de los datos podemos utilizar las funciones **str()**, **summary()** y **dim()**.

```{r}
dim(complete_df)
summary(complete_df)
str(complete_df)
```

Bien, vemos que la base de datos completa tiene 1309 observaciones y 13 variables. 

### 2.3. Limpieza de los datos. 

Una de las cosas importantes a la hora de importar los datos es ver si R ha asignado correctamente la categoría a cada varaiable. Por ejemplo, si nos fijamos en la variable *Survived* podemos ver que para R es una varaiable de tipo integer (numérico), pero quizás sería mejor que fuera un factor con dos niveles, donde 0 indique que no sobrevivió y 1 indique que sí sobrevivió. Así mismo, la variable *Pclass* también sería un factor, en este caso un factor ordenado ya que es una variable ordinal.

#### 2.3.1. ¿Los datos contienen ceros o elementos vacíos? ¿Cómo gestionarías cada uno de estos casos? 

Tenemos tres variables con datos perdidos o elementos vacios, las variables: *age*, donde los valores perdidos están indicados con "NA", y las variables *Cabin* y *Embarked*, que están caracterizados como factores y tienen un nivel "" que indica un valor perdido. De manera adicional, tenemos la variable *Fare*, que es el precio del ticket que contiene ceros... este valor en principio parece raro, pero podría ser por ejemplo que identificara a los tripulantes y no a los pasajeros... por lo tanto lo tendremos que investigar más a fondo. Las variables *Survived*, *SibSp* y *Psrch* también contienen ceros, pero esos valores no son valores perdidos y tienen un sentido claro (que no sobrevivió, que no tenía hermanos/as ni esposo/a a bordo, y que no tenía padres ni hijos/as a bordo).



#### 2.3.2. Identificación y tratamiento de valores extremos. 

### 2.4. Análisis de los datos. 


#### 2.4.1. Selección de los grupos de datos que se quieren analizar/comparar (planificación de los análisis a aplicar).


#### 2.4.2. Comprobación de la normalidad y homogeneidad de la varianza. 



#### 2.4.3. Aplicación de pruebas estadísticas para comparar los grupos de datos. En función de los datos y el objetivo del estudio, aplicar pruebas de contraste de hipótesis, correlaciones, regresiones, etc. Aplicar al menos tres métodos de análisis diferentes. 

### 2.5. Representación de los resultados a partir de tablas y gráficas. 

### 2.6. Resolución del problema. A partir de los resultados obtenidos, ¿cuáles son las conclusiones? ¿Los resultados permiten responder al problema? 


### 2.7. Código: Hay que adjuntar el código, preferiblemente en R, con el que se ha realizado la limpieza, análisis y representación de los datos. Si lo preferís, también podéis trabajar en Python.